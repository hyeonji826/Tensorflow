{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd5JFYp8Hjp/dgb8ENoyWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonji826/Tensorflow/blob/main/Course1_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 신경망 모델 정의\n",
        "- Sequential 모델 : 여러 레이어를 순차적으로 쌓아 올리는 방식입니다.\n",
        "\n",
        "- Dense 레이어 : 하나의 Dense 레이어를 사용하며, 이 레이어는 1개의 뉴런을 포함하고 있고 입력으로 1개의 값만 받습니다.\n",
        "\n",
        "```r\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "```\n"
      ],
      "metadata": {
        "id": "l49W5d8A0e42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 모델 컴파일\n",
        "- Optimizer (SGD) : 확률적 경사 하강법(Stochastic Gradient Descent)을 사용해 모델의 가중치를 업데이트하여 손실을 줄인다.\n",
        "\n",
        "- Loss 함수 (Mean Squared Error) : 모델의 예측값과 실제 값 사이의 차이를 제곱하여 평균을 낸 값을 손실로 사용한다.\n",
        "\n",
        "모델을 학습과정 중 이 손실 값을 최소화하기 위해 가중치를 조정하게 된다.\n",
        "\n",
        "```r\n",
        "model.compile(optimizer='sgd',loss='mean_squared_error')\n",
        "```"
      ],
      "metadata": {
        "id": "W6KmZC7q0_lV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 데이터 제공\n",
        "- 입력 데이터(xs) : -1.0부터 4.0까지 6개의 값을 입력 데이터로 사용한다.\n",
        "\n",
        "- 출력 데이터(ys) : 각 입력값에 대해 y=2x-1의 관계에 따라 계산된 값들을 출력 데이터로 사용한다.\n",
        "Ex) x = -1.0 이면 y = -3.0\n",
        "\n",
        "```r\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0],dtype=float)\n",
        "```\n"
      ],
      "metadata": {
        "id": "Jxl6w6BX1nES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 신경망 학습\n",
        "- 모델 학습(fit) : **model.fit** 함수를 사용해 네트워크가 입력과 출력 데이터를 통해 학습하도록 한다.\n",
        "\n",
        "- 에포크(epoch) : 전체 데이터셋을 한번씩 학습하는 것을 하나의 에포크라고 하며, 여러번 반복하면서 네트워크가 점점 더 정확하게 관계를 학습하게 된다.\n",
        "\n",
        "```r\n",
        "model.fit(xs,ys,epcohs=500)\n",
        "```"
      ],
      "metadata": {
        "id": "Wgu-QaL-2Wum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 예측 수행\n",
        "- 모델 예측(predict) : 학습된 모델을 이용해 새로운 입력값에 대한 출력을 예측할 수 있다.\n",
        "\n",
        "- 예측 결과의 미세한 차이 : 실제 예측 결과가 약간 다를 수 있는데, 이는 학습 데이터의 갯수가 한정적이고 모델이 확률적 방법으로 근사값을 학습하기 때문이다.\n",
        "\n",
        "```r\n",
        "print(model.predict([10.0]))\n",
        "```"
      ],
      "metadata": {
        "id": "Jtuxz5WZ2zfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hello World Layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "layer_0 = keras.layers.Dense(units=1,input_shape=[1])\n",
        "model = tf. keras.Sequential([layer_0])\n",
        "\n",
        "model.compile(optimizer='sgd',loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0],dtype=float)\n",
        "\n",
        "model.fit(xs,ys,epochs=500)\n",
        "\n",
        "print(model.predict([10.0]))\n",
        "\n",
        "print(\"Layer variable look like this : {}\".format(layer_0.get_weights()))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BwL-obQO3RZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}